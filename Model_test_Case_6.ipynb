{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-25 20:18:03.857189: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-25 20:18:04.398472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RepeatedKFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataSolarModules = pd.read_json('InfraredSolarModules/module_metadata.json').transpose().sort_index()\n",
    "DataSolarModulesGT = pd.read_json('InfraredSolarModules/GT_metadata.json').transpose().sort_index()\n",
    "DataSolarModulesVAE = pd.read_json('InfraredSolarModules/VAE_metadata.json').transpose().sort_index()\n",
    "Classes = DataSolarModules['anomaly_class'].unique()\n",
    "class_to_number = dict(enumerate(Classes.flatten(), 0))\n",
    "class_to_number = {v: k for k, v in class_to_number.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_class(value):\n",
    "    class_to_number\n",
    "    return class_to_number.get(value, 'Unknown')\n",
    "\n",
    "DataSolarModules['class_code'] = DataSolarModules['anomaly_class'].apply(map_to_class)\n",
    "DataSolarModulesGT['class_code'] = DataSolarModulesGT['anomaly_class'].apply(map_to_class)\n",
    "DataSolarModulesVAE['class_code'] = DataSolarModulesVAE['anomaly_class'].apply(map_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images_dataframe(dataframe):\n",
    "    images = []\n",
    "    for image_path in dataframe['image_filepath']:\n",
    "        img = cv2.imread(\"InfraredSolarModules/\"+image_path,cv2.IMREAD_GRAYSCALE)\n",
    "        img = img.reshape(40, 24).astype(\"float32\") / 255\n",
    "        images.append(img)\n",
    "    images=np.array(images) \n",
    "    return images\n",
    "\n",
    "def read_labels_dataframe(dataframe):\n",
    "    labels = []\n",
    "    for label in dataframe['class_code']:\n",
    "        labels.append(label)\n",
    "    labels=np.array(labels) \n",
    "    labels.astype(\"int32\")   \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_OG = read_images_dataframe(DataSolarModules)\n",
    "images_GT = read_images_dataframe(DataSolarModulesGT)\n",
    "images_VAE = read_images_dataframe(DataSolarModulesVAE)\n",
    "labels_OG = read_labels_dataframe(DataSolarModules)\n",
    "labels_GT = read_labels_dataframe(DataSolarModulesGT)\n",
    "labels_VAE = read_labels_dataframe(DataSolarModulesVAE)\n",
    "\n",
    "images=np.concatenate((images_OG, images_GT,images_VAE), axis=0)\n",
    "labels=np.concatenate((labels_OG, labels_GT,labels_VAE), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a testing set with 25 images per class\n",
    "num_images_per_class = 40\n",
    "\n",
    "def create_testing_set(dataframe, num_images_per_class):\n",
    "    test_indices = []\n",
    "    for cls in dataframe['class_code'].unique():\n",
    "        class_indices = dataframe[dataframe['class_code'] == cls].index[:num_images_per_class].tolist()\n",
    "        test_indices.extend(class_indices)\n",
    "    return test_indices\n",
    "\n",
    "test_indices = create_testing_set(DataSolarModules, num_images_per_class)\n",
    "data_test = np.array(test_indices)\n",
    "\n",
    "# Create a training set with the remaining images\n",
    "data_train = np.setdiff1d(np.arange(len(images)), data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample specified amount of images per class for training set\n",
    "def sample_training_set(dataframe, samples):\n",
    "    train_indices = []\n",
    "    for cls, num_samples in enumerate(samples):\n",
    "        class_indices = dataframe[dataframe['class_code'] == cls].index[num_images_per_class:]\n",
    "        sampled_indices = class_indices[:num_samples].tolist()\n",
    "        train_indices.extend(sampled_indices)\n",
    "    return train_indices\n",
    "\n",
    "\n",
    "\n",
    "samples_OG = [12, 21, 23, 23, 12, 43, 76, 54, 87, 23, 98, 12]\n",
    "samples_VAE = [12, 21, 23, 23, 12, 43, 76, 54, 87, 23, 98, 12]\n",
    "samples_GT = [12, 21, 23, 23, 12, 43, 76, 54, 87, 23, 98, 12]\n",
    "\n",
    "train_indices_OG = sample_training_set(DataSolarModules, samples_OG)\n",
    "train_indices_VAE = sample_training_set(DataSolarModulesVAE, samples_VAE)\n",
    "train_indices_GT = sample_training_set(DataSolarModulesGT, samples_GT)\n",
    "train_indices = np.concatenate((train_indices_OG, train_indices_VAE, train_indices_GT), axis=0)\n",
    "\n",
    "data_train = np.array(train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(40, 24, 1))\n",
    "\n",
    "\n",
    "x = layers.Conv2D(filters=16, kernel_size=3, activation=\"relu\")(inputs)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "x = layers.Dense(2048, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.4, noise_shape=None, seed=None)(x)\n",
    "x = layers.Dense(1024, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.4, noise_shape=None, seed=None)(x)\n",
    "x = layers.Dense(512, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.4, noise_shape=None, seed=None)(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.4, noise_shape=None, seed=None)(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "\n",
    "\n",
    "outputs = layers.Dense(12, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n_epochs = 25\n",
    "\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "rkf = RepeatedKFold(n_splits=6, n_repeats=3, random_state=21312312)\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer,                                    \n",
    "              loss=\"sparse_categorical_crossentropy\",                 \n",
    "              metrics=[\"accuracy\"])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_no = 1\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "all_train_losses = []\n",
    "all_train_accuracies = []\n",
    "all_val_losses = []\n",
    "all_val_accuracies = []\n",
    "\n",
    "for train, val in rkf.split(data_train):\n",
    "\n",
    "    history_model = model.fit(images[train], labels[train],\n",
    "                              epochs=n_epochs,\n",
    "                              validation_data=(images[val], labels[val]),\n",
    "                              batch_size=batch_size)\n",
    "\n",
    "    # Store metrics for this fold\n",
    "    fold_train_loss = np.mean(history_model.history['loss'])\n",
    "    fold_train_accuracy = np.mean(history_model.history['accuracy'])\n",
    "    fold_val_loss = np.mean(history_model.history['val_loss'])\n",
    "    fold_val_accuracy = np.mean(history_model.history['val_accuracy'])\n",
    "\n",
    "    all_train_losses.append(fold_train_loss)\n",
    "    all_train_accuracies.append(fold_train_accuracy)\n",
    "    all_val_losses.append(fold_val_loss)\n",
    "    all_val_accuracies.append(fold_val_accuracy)\n",
    "\n",
    "    # Generate generalization metrics for the last epoch\n",
    "    scores = model.evaluate(images[val], labels[val],verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1] * 100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plotting average training and validation losses\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(range(1, len(all_train_losses) + 1), all_train_losses, color='blue', label='Training')\n",
    "plt.plot(range(1, len(all_val_losses) + 1), all_val_losses, color='orange', label='Validation')\n",
    "plt.title('Average Training and Validation Loss')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting average training and validation accuracies\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(range(1, len(all_train_accuracies) + 1), [acc * 100 for acc in all_train_accuracies], color='green', label='Training')\n",
    "plt.plot(range(1, len(all_val_accuracies) + 1), [acc * 100 for acc in all_val_accuracies], color='red', label='Validation')\n",
    "plt.title('Average Training and Validation Accuracy')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = model.evaluate(images[data_test], labels[data_test])  \n",
    "print(\"Test loss:\", test_metrics[0])\n",
    "print(\"Test accuracy:\", test_metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Confusion Matrix\n",
    "predictions = model.predict(images[data_test], steps=len(data_test), verbose=0)\n",
    "\n",
    "y_pred = np.argmax(predictions, axis=-1)\n",
    "\n",
    "cm = confusion_matrix(labels[data_test], y_pred)\n",
    "\n",
    "## Get Class Labels\n",
    "class_names = Classes\n",
    "\n",
    "# Plot confusion matrix in a beautiful manner\n",
    "fig = plt.figure(figsize=(16, 14))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True,robust=True, ax = ax, fmt = 'g'); #annot=True to annotate cells\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted', fontsize=20)\n",
    "ax.xaxis.set_label_position('bottom')\n",
    "plt.xticks(rotation=90)\n",
    "ax.xaxis.set_ticklabels(class_names, fontsize = 10)\n",
    "ax.xaxis.tick_bottom()\n",
    "\n",
    "ax.set_ylabel('True', fontsize=20)\n",
    "ax.yaxis.set_ticklabels(class_names, fontsize = 10)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.title('Refined Confusion Matrix', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_and_fn = cm.sum(1)\n",
    "tp_and_fp = cm.sum(0)\n",
    "tp = cm.diagonal()\n",
    "accuracy = cm.diagonal().sum()/cm.sum()\n",
    "precision = tp / tp_and_fp\n",
    "recall = tp / tp_and_fn\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "precision_avg=np.average(precision)\n",
    "recall_avg=np.average(recall)\n",
    "f1_avg=np.average(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary with the data\n",
    "data = {\n",
    "    'Metrics': ['Accuracy', 'Average Precision', 'Average Recall', 'Average F1 Score'],\n",
    "    'Values': [accuracy, precision_avg, recall_avg, f1_avg]\n",
    "}\n",
    "\n",
    "# Creating the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Displaying the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Class': class_names,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1\n",
    "}\n",
    "\n",
    "# Creating the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Displaying the DataFrame\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.Keras': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c6bf8e3c71d29b289656a0e522ae20919f8da47e1e0df367350538eb0f05593"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
